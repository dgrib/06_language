{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUotKHRULVPD"
   },
   "source": [
    "# Инструменты для работы с языком "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ba5Z02VLVPK"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
    "\n",
    "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zHB70n5LVPN",
    "outputId": "76e9cc41-c912-464c-a06e-e02879e69c08",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
    "# !wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "# !wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J5YiZNCPLVPe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DFLtXAZ-LVPq"
   },
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "j1AEISlBLVP0",
    "outputId": "443eadf2-9df4-4507-f2a5-a64f7968182f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZWta7oDgLVP8"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAapBC7VLVQC"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n",
    "\n",
    "### Векторизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M-AvVt8XLVQD"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSuoVoxcLVQI"
   },
   "source": [
    "Что такое n-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zeNA7732LVQJ"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeApDOmrLVQN",
    "outputId": "d8c763fe-2658-47ac-fcee-5b7bbe49f0d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Если б мне платили каждый раз'.split()\n",
    "list(ngrams(sent, 1)) # униграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPAS0fS-LVQQ",
    "outputId": "aa3ae031-c661-4639-b7ab-f93ba1b6cee5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б'),\n",
       " ('б', 'мне'),\n",
       " ('мне', 'платили'),\n",
       " ('платили', 'каждый'),\n",
       " ('каждый', 'раз')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d77jmVPhLVQU",
    "outputId": "c8de801b-8efc-437e-8673-4e9e31665ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне'),\n",
       " ('б', 'мне', 'платили'),\n",
       " ('мне', 'платили', 'каждый'),\n",
       " ('платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xXTBrGELVQX",
    "outputId": "f5b423d9-db6b-4efa-8bfa-a446a55ee018"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
       " ('б', 'мне', 'платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 5)) # ... пентаграммы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHGJBEm-LVQb"
   },
   "source": [
    "Самый простой способ извлечь фичи из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
    "\n",
    "Объект `CountVectorizer` делает простую вещь:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
    "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eMqZFBTgLVQb"
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train) # bow -- bag of words (мешок слов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZkpqVtILVQe"
   },
   "source": [
    "ngram_range отвечает за то, какие n-граммы мы используем в качестве фичей:<br/>\n",
    "ngram_range=(1, 1) -- униграммы<br/>\n",
    "ngram_range=(3, 3) -- триграммы<br/>\n",
    "ngram_range=(1, 3) -- униграммы, биграммы и триграммы.\n",
    "\n",
    "В vec.vocabulary_ лежит словарь: мэппинг слов к их индексам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWRtOSzKLVQf",
    "outputId": "668857b0-def2-4547-8fd5-014fe3858bec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dj_kafe', 26066),\n",
       " ('тебя', 220794),\n",
       " ('то', 222115),\n",
       " ('взрослее', 112219),\n",
       " ('эта', 241509),\n",
       " ('это', 241574),\n",
       " ('пздц', 182036),\n",
       " ('нытик', 169751),\n",
       " ('ууу', 229209),\n",
       " ('сдохну', 207951)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vec.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkmX3iBbLVQi",
    "outputId": "5bbb432e-1c45-422a-edc2-60841ef0ee33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJ8q5_59LVQm",
    "outputId": "d88c6de2-640a-4cc9-ae95-29fe12c80131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.76      0.76     27641\n",
      "    positive       0.77      0.76      0.77     29068\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAhgaYgqLVQp"
   },
   "source": [
    "Попробуем сделать то же самое для триграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPWXlh6ALVQq",
    "outputId": "094a7189-bc36-42df-81ee-f599a206ca0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.57      0.68     41348\n",
      "    positive       0.39      0.72      0.50     15361\n",
      "\n",
      "    accuracy                           0.61     56709\n",
      "   macro avg       0.62      0.65      0.59     56709\n",
      "weighted avg       0.72      0.61      0.63     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(3, 3))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnfyJkzTLVQu"
   },
   "source": [
    "(как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJABxhalLVQu"
   },
   "source": [
    "## TF-IDF векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LJES2s-LVQv"
   },
   "source": [
    "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений – tf-idf каждого слова.\n",
    "\n",
    "Как считается tf-idf:\n",
    "\n",
    "TF (term frequency) – относительная частотность слова в документе:\n",
    "$$ TF(t,d) = \\frac{n_t}{\\sum_k n_k} $$\n",
    "\n",
    "`t` -- слово (term), `d` -- документ, $n_t$ -- количество вхождений слова, $n_k$ -- количество вхождений остальных слов\n",
    "\n",
    "IDF (inverse document frequency) – обратная частота документов, в которых есть это слово:\n",
    "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
    "\n",
    "`t` -- слово (term), `D` -- коллекция документов\n",
    "\n",
    "Перемножаем их:\n",
    "$$TFIDF_(t,d,D) = TF(t,d) \\times IDF(i, D)$$\n",
    "\n",
    "Сакральный смысл – если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
    "количестве документов, у него высокий TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FmEcRD28LVQ0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWLhMl9xLVQ3",
    "outputId": "054e5662-1c41-42f6-92e4-ce0194317ce8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.77      0.75     26756\n",
      "    positive       0.78      0.75      0.77     29953\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTODTRnKLVQ6"
   },
   "source": [
    "В этот раз получилось хуже :( Вернёмся к `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8v9Scpn9Y0M"
   },
   "source": [
    "## PMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVRqLcSY0etj"
   },
   "source": [
    "Можно оценить взаимосвязь слов в корпусе и понять, какие биграммы наиболее часто встречаются в тексте. Для этого можно использовать метрику PMI (Pointwise Mutual Information) - поточечная взаимная информация. Метрика PMI для двух слов вычисляется по формуле:\n",
    "\n",
    "$$pmi(x; y) = log \\frac{p(x,y)}{p(x)p(y)} $$\n",
    "\n",
    "Здесь p(y|x) - вероятность встретить слово $y$ после $x$, $p(y)$ - вероятность встретить слово $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXgDwf6W6Kk5"
   },
   "source": [
    "Оценим важность биграмм в нашем обучающем корпусе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKmiOEaW53F9",
    "outputId": "96a85968-e0cb-4b17-cd62-bc3de5e3e9b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\gribanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\genesis.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "[('+1239', '728'), ('+СОННО', '+НЕ'), ('+живіт', 'болить.ну'), ('+погода', 'крутая='), (',4', 'запирайте'), (',Дела', 'рез'), ('-/////', 'прбрм-прбрм'), ('-10,11', 'болсо'), ('-53', 'dBm'), ('-700', 'рублей.-А'), ('-АХАХАХАХ', 'ЮБКУ'), ('-АХАХАХАХХАХАХАХАХАХХА', '-АХАХАХХАХАХАХАХАХ'), ('-Айгуль', 'Маратовна'), ('-Аха', 'спетросянил'), ('-ВСЕМ', 'СПОКОЙНЫХ'), ('-Весело', 'кншн:3'), ('-Восьмигрудый', 'трипи'), ('-Время', 'эмокора'), ('-Выздоравливай', 'педрилк'), ('-Д-Д-Д-Д-Д-Д-ДРОП', 'ЗЭ'), ('-ДЕТЕЙ', 'НАКРЫЛО'), ('-ДОВАЙТИ', 'АЛДСКУЛ'), ('-Дирол', 'Сенсес'), ('-Домашка', '-кл.час'), ('-ЖАРЕНЫЙ', 'КАРТОФЕЛЬ'), ('-ЖРАТЬ', 'БАРАНКИ'), ('-ЗАШЛА', 'ОДЕЛА'), ('-Защитано', '-ес'), ('-Зелено-карие', '-Киллджой'), ('-Керем', 'севгили'), ('-Киллджой', '-Котик'), ('-Корнейчук', 'затроллила'), ('-МНОГО', 'СЛИВОК'), ('-Маладец', '-Лол'), ('-Мамаааа', 'поправь'), ('-НА', 'РЕАЛЬНЫХ'), ('-Настя', 'Пармсон'), ('-ОЗВУЧИВАТЕЛЬ', 'МУЛЬТИКОВ'), ('-ОНИ', 'СТОЯТ'), ('-Олесь', '-Пошёл'), ('-Песня', 'грусная='), ('-Плохое', 'пищеварение'), ('-Поэзия', 'заключает'), ('-Рыбу', 'соленую'), ('-СУКА', 'ЛУКАШИН'), ('-Танцы', '-Хорошие'), ('-Ти', 'кантужена'), ('-Тиць', 'дурне'), ('-Трахався', '-Іди'), ('-ФОН', '-БИО'), ('-Филл', '-познакомились'), ('-ШАПКА', '-ФОН'), ('-ЮН', '-ЮП'), ('-ЮП', '-ШАПКА'), ('-Юлия', 'Зазулина'), ('-Я.банан', '-ахх'), ('-анал', '-абстиненция'), ('-анатолий', 'николаевич'), ('-бляя', 'хммммммм'), ('-водичку', 'лью'), ('-г', 'үзээ'), ('-гoop', 'Хакүхо'), ('-говорит', 'одноногий'), ('-дерьмо', 'редкосное'), ('-иногда', '.Ностальгирую'), ('-киллджой', '-шатенка'), ('-классный', 'фильм.правда'), ('-кучи', 'мутики'), ('-ладно', '-АХАХАХАХХАХАХАХАХАХХА'), ('-любому', 'умрёшь'), ('-место', '-кровать'), ('-напиши', '-нит'), ('-патлатые', '-лысый'), ('-пиздишь', '-отвечаю'), ('-попробуй', 'помедитировать'), ('-раз', 'плёткой'), ('-распускаю', 'волосы-'), ('-руу', 'орох'), ('-рцензию', 'десятилетия'), ('-случайные', 'вопросы-'), ('-спрашивает', 'жена.-У'), ('-ступайте', 'сани'), ('-такое', 'ВЧ-8='), ('-то.Даже', 'старух'), ('-указуказуказуказуа', '-материшся'), ('-хаски', '-розовое'), ('-хуи', 'сосешь'), ('-хуле', 'депрессушный'), ('-цытата', 'Шимы'), ('-черные', '-аниме'), ('-шапку', '-фон'), ('-ыг', 'үгүйсгэж'), ('-ынхаа', 'кодын'), ('-эртага', 'бозорга'), ('-юн', '-юп'), ('-юп', '-шапку'), ('-языком', 'владеешь'), ('.NET', 'языки.'), ('.Вы', 'ахуеете'), ('.Спасибо', 'Бердянску')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import collocations \n",
    "nltk.download('genesis')\n",
    "\n",
    "print(type(nltk.corpus.genesis.words('english-web.txt')))\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "# bigram_finder.apply_freq_filter(5)\n",
    "bigram_finder = collocations.BigramCollocationFinder.from_documents([nltk.word_tokenize(x) for x in x_train])\n",
    "bigrams = bigram_finder.nbest(bigram_measures.pmi, 100)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h4Cq1PUTTc-"
   },
   "source": [
    "Можно рассмотреть другие метрики оценки важности биграмм, например, метрику правдоподобия (подробнее про вычисление метрики можно посмотреть [здесь (пункт 5.3.4)](http://www.corpus.unam.mx/cursoenah/ManningSchutze_1999_FoundationsofStatisticalNaturalLanguageProcessing.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTOJg4KoOo84",
    "outputId": "45d38953-84e0-4a65-fccd-1e96fd83c3f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', '('), ('RT', '@'), (')', ')'), ('http', ':'), ('!', '!'), (':', 'D'), ('у', 'меня'), (':', '('), (',', 'а'), (',', 'что'), (',', 'но'), (')', 'http'), ('*', '*'), (':', ')'), ('(', ','), ('у', 'нас'), (',', '('), (':', '-'), ('не', 'могу'), (',', ')'), ('?', '?'), (')', ','), (',', ':'), ('@', '('), (',', ','), ('(', ':'), (':', ','), ('@', ')'), ('@', ','), ('&', 'lt'), ('со', 'мной'), ('@', ':'), ('(', '@'), (':', ':'), ('новый', 'год'), (';', ')'), ('gt', ';'), (')', ':'), ('не', 'знаю'), (',', '@'), (':', '*'), ('а', 'я'), ('@', '@'), (',', 'когда'), ('потому', 'что'), ('У', 'меня'), ('сих', 'пор'), ('lt', ';'), ('&', 'gt'), ('у', 'тебя'), (';', '('), ('все', 'равно'), (',', 'как'), ('с', 'тобой'), ('в', 'школу'), ('ничего', 'не'), ('&', 'amp'), ('Как', 'же'), ('Доброе', 'утро'), ('(', 'http'), (',', 'я'), (')', '@'), ('-', ')'), ('я', 'не'), ('--', '--'), (':', 'DD'), ('не', '('), ('самом', 'деле'), ('не', ')'), ('как', 'же'), ('amp', ';'), ('(', '!'), ('до', 'сих'), (',', 'чтобы'), ('об', 'этом'), ('что', 'я'), (',', '!'), ('не', ':'), ('D', 'http'), ('с', 'кем'), ('и', '('), ('никто', 'не'), ('=', ')'), (':', '!'), ('!', ','), ('а', 'потом'), ('.', 'А'), ('?', '—'), (':', '|'), ('Новый', 'Год'), ('никогда', 'не'), ('и', ')'), (',', '.'), ('.', ','), ('.', 'Но'), ('@', 'не'), ('не', '@'), ('#', 'євромайдан'), ('=', '('), ('“', '@')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = bigram_finder.nbest(bigram_measures.likelihood_ratio, 100)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfjCYZa8TeX_"
   },
   "source": [
    "Как можно заметить, немаловажную роль в текстах занимает пунктуация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AJk1B39LVRP"
   },
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "*Стоп-слова* -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpWhsTuRLVRP",
    "outputId": "1cd18efe-a3cd-4f56-ec4c-bec8b6ee442e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gribanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# у вас здесь, вероятно, выскочит ошибка и надо будет загрузить стоп слова (в тексте ошибки написано, как)\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OdRF7rlyLVRS",
    "outputId": "dd4ce4f0-13d0-4b21-a3a1-b9ecb281894f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OfXiH98XLVRV"
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtiIhHDMLVRY"
   },
   "source": [
    "В векторизаторах за стоп-слова, логичным образом, отвечает аргумент `stop_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZnbarm_LVRY",
    "outputId": "88f40278-165a-46ad-a75e-2e5d8e7e3a4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.76      0.78     29347\n",
      "    positive       0.76      0.80      0.78     27362\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr934O7yLVRb"
   },
   "source": [
    "Получилось чууть лучше. Что ещё можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7O_oD1fLVRc"
   },
   "source": [
    "## Лемматизация\n",
    "\n",
    "Лемматизация – это сведение разных форм одного слова к начальной форме – *лемме*. Почему это хорошо?\n",
    "* Во-первых, мы хотим рассматривать как отдельную фичу каждое *слово*, а не каждую его отдельную форму.\n",
    "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть два хороших лемматизатора: mystem и pymorphy:\n",
    "\n",
    "### [Mystem](https://tech.yandex.ru/mystem/)\n",
    "Как с ним работать:\n",
    "* можно скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
    "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) - обертка для питона, работает медленнее, но это удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96HdoB7zLVRc",
    "outputId": "987397bc-55cb-4830-c361-5568f1f2e015"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "tar: Error opening archive: Failed to open 'mystem-3.0-linux3.1-64bit.tar.gz'\n",
      "\"cp\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!cp mystem /bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kzQwGwAaZWV5"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w-_fkNtLVRf"
   },
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "* mystem_bin - путь к `mystem`, если их несколько\n",
    "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
    "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
    "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjHHLQv9txDq",
    "outputId": "35bcb3cc-7853-48bf-d38d-04157f45221d"
   },
   "outputs": [],
   "source": [
    "# print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI1eftjkLVRi"
   },
   "source": [
    "А можно получить грамматическую информацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4MLqlZnxNEj",
    "outputId": "ffb676bb-932e-4579-817a-c15616431521"
   },
   "outputs": [],
   "source": [
    "# mystem_analyzer.analyze(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADcGtz4JLVRl"
   },
   "source": [
    "Давайте терепь используем лемматизатор майстема в качестве токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "x48Q56tiLVRn"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def my_preproc(text):\n",
    "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "    text = mystem_analyzer.lemmatize(text)\n",
    "    return [word for word in text if word not in stopwords.words('russian') + [' ', '\\n']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEwOQTJPLVRq",
    "outputId": "b1047a05-9fa7-4994-c947-578cfc4d9825"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# vec = CountVectorizer(ngram_range=(1, 1), tokenizer=my_preproc)\n",
    "# bow = vec.fit_transform(x_train)\n",
    "# clf = LogisticRegression(random_state=42)\n",
    "# clf.fit(bow, y_train)\n",
    "# pred = clf.predict(vec.transform(x_test))\n",
    "# print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJlvqWuALVRs"
   },
   "source": [
    "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHDkurN1zf7g",
    "outputId": "c9934446-bf72-4603-8a51-9f6ebf406e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from pymorphy2) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7SlwsLU7LVRt"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qaz0x7frLVRw"
   },
   "source": [
    "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdf6XoEbLVRw",
    "outputId": "a1984e06-dbeb-4377-896d-b7014b6b84c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='платили', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='платить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'платили', 2472, 10),))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = pymorphy2_analyzer.parse(sent[3])\n",
    "ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0KuHQGPgLVRz",
    "outputId": "3cdbe79d-ec3f-4a07-ff3a-52e8810face1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'платить'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gg0EASPcLVR8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFTkF8xUARlS"
   },
   "source": [
    "### [Natasha](https://github.com/natasha/)\n",
    "\n",
    "В библиотеке natasha реализовано множество полезных библиотек для русского языка: разбиение на токены и предложения, русскоязычные word embeddings, морфологический, синтаксический анализ, лемматизация, извлечение именованных сущностей и т.д. Модуль библиотеки Razdel, основанный на правилах, предназначен для разбиения текста на токены и предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CVeDxeIA6rg",
    "outputId": "ff583009-ae7a-4b68-b6a7-ca1ba48c0732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOTkw9MpAnNN",
    "outputId": "51fc6e54-3b82-4c1c-91c2-5b6a60f6304f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 13, 'Кружка-термос'),\n",
       " Substring(14, 16, 'на'),\n",
       " Substring(17, 20, '0.5'),\n",
       " Substring(20, 21, 'л'),\n",
       " Substring(22, 23, '('),\n",
       " Substring(23, 28, '50/64'),\n",
       " Substring(29, 32, 'см³'),\n",
       " Substring(32, 33, ','),\n",
       " Substring(34, 37, '516'),\n",
       " Substring(37, 38, ';'),\n",
       " Substring(38, 41, '...'),\n",
       " Substring(41, 42, ')')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ftx-WzUbBCpO",
    "outputId": "40c67fa4-fab6-4c1b-f651-06951a38798e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Кружка-термос',\n",
       " 'на',\n",
       " '0.5',\n",
       " 'л',\n",
       " '(',\n",
       " '50/64',\n",
       " 'см³',\n",
       " ',',\n",
       " '516',\n",
       " ';',\n",
       " '...',\n",
       " ')']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyhsQp4MGbW8",
    "outputId": "55f84576-aaad-4a68-c5d9-38dc0ea2f721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
      "Collecting navec>=0.9.0\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: razdel>=0.5.0 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from natasha) (0.5.0)\n",
      "Collecting slovnet>=0.3.0\n",
      "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
      "Collecting yargy>=0.14.0\n",
      "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: pymorphy2 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from natasha) (0.9.1)\n",
      "Collecting ipymarkup>=0.8.0\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from navec>=0.9.0->natasha) (1.18.5)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: intervaltree>=3 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\gribanov\\anaconda3\\lib\\site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
      "Installing collected packages: navec, slovnet, yargy, ipymarkup, natasha\n",
      "Successfully installed ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 slovnet-0.5.0 yargy-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMO3jsqLKSIV"
   },
   "source": [
    "С помощью библиотеки natasha можно также лемматизировать тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "vJZgfRnvIS2q"
   },
   "outputs": [],
   "source": [
    "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "def natasha_lemmatize(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    return {_.text: _.lemma for _ in doc.tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBtlnYlFBOKv",
    "outputId": "47a9e7d6-7f03-4e93-e57c-84dce5657d97",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Посол': 'посол',\n",
       " 'Израиля': 'израиль',\n",
       " 'на': 'на',\n",
       " 'Украине': 'украина',\n",
       " 'Йоэль': 'йоэль',\n",
       " 'Лион': 'лион',\n",
       " 'признался': 'признаться',\n",
       " ',': ',',\n",
       " 'что': 'что',\n",
       " 'пришел': 'прийти',\n",
       " 'в': 'в',\n",
       " 'шок': 'шок',\n",
       " 'узнав': 'узнать',\n",
       " 'о': 'о',\n",
       " 'решении': 'решение',\n",
       " 'властей': 'власть',\n",
       " 'Львовской': 'львовский',\n",
       " 'области': 'область',\n",
       " 'объявить': 'объявить',\n",
       " '2019': '2019',\n",
       " 'год': 'год',\n",
       " 'годом': 'год',\n",
       " 'лидера': 'лидер',\n",
       " 'запрещенной': 'запретить',\n",
       " 'России': 'россия',\n",
       " 'Организации': 'организация',\n",
       " 'украинских': 'украинский',\n",
       " 'националистов': 'националист',\n",
       " '(': '(',\n",
       " 'ОУН': 'оун',\n",
       " ')': ')',\n",
       " 'Степана': 'степан',\n",
       " 'Бандеры': 'бандера',\n",
       " '.': '.',\n",
       " 'Свое': 'свой',\n",
       " 'заявление': 'заявление',\n",
       " 'он': 'он',\n",
       " 'разместил': 'разместить',\n",
       " 'Twitter': 'twitter',\n",
       " '«': '«',\n",
       " 'Я': 'я',\n",
       " 'не': 'не',\n",
       " 'могу': 'мочь',\n",
       " 'понять': 'понять',\n",
       " 'как': 'как',\n",
       " 'прославление': 'прославление',\n",
       " 'тех': 'тот',\n",
       " 'кто': 'кто',\n",
       " 'непосредственно': 'непосредственно',\n",
       " 'принимал': 'принимать',\n",
       " 'участие': 'участие',\n",
       " 'ужасных': 'ужасный',\n",
       " 'антисемитских': 'антисемитский',\n",
       " 'преступлениях': 'преступление',\n",
       " 'помогает': 'помогать',\n",
       " 'бороться': 'бороться',\n",
       " 'с': 'с',\n",
       " 'антисемитизмом': 'антисемитизм',\n",
       " 'и': 'и',\n",
       " 'ксенофобией': 'ксенофобия',\n",
       " 'Украина': 'украина',\n",
       " 'должна': 'должный',\n",
       " 'забывать': 'забывать',\n",
       " 'совершенных': 'совершить',\n",
       " 'против': 'против',\n",
       " 'евреев': 'еврей',\n",
       " 'никоим': 'никой',\n",
       " 'образом': 'образ',\n",
       " 'отмечать': 'отмечать',\n",
       " 'их': 'они',\n",
       " 'через': 'через',\n",
       " 'почитание': 'почитание',\n",
       " 'исполнителей': 'исполнитель',\n",
       " '»': '»',\n",
       " '—': '—',\n",
       " 'написал': 'написать',\n",
       " 'дипломат': 'дипломат',\n",
       " '11': '11',\n",
       " 'декабря': 'декабрь',\n",
       " 'Львовский': 'львовский',\n",
       " 'областной': 'областной',\n",
       " 'совет': 'совет',\n",
       " 'принял': 'принять',\n",
       " 'решение': 'решение',\n",
       " 'провозгласить': 'провозгласить',\n",
       " 'регионе': 'регион',\n",
       " 'связи': 'связь',\n",
       " 'празднованием': 'празднование',\n",
       " '110-летия': '110-летие',\n",
       " 'со': 'с',\n",
       " 'дня': 'день',\n",
       " 'рождения': 'рождение',\n",
       " 'Бандера': 'бандера',\n",
       " 'родился': 'родиться',\n",
       " '1': '1',\n",
       " 'января': 'январь',\n",
       " '1909': '1909',\n",
       " 'года': 'год',\n",
       " 'В': 'в',\n",
       " 'июле': 'июль',\n",
       " 'аналогичное': 'аналогичный',\n",
       " 'Житомирский': 'житомирский',\n",
       " 'начале': 'начало',\n",
       " 'месяца': 'месяц',\n",
       " 'предложением': 'предложение',\n",
       " 'к': 'к',\n",
       " 'президенту': 'президент',\n",
       " 'страны': 'страна',\n",
       " 'Петру': 'петр',\n",
       " 'Порошенко': 'порошенко',\n",
       " 'вернуть': 'вернуть',\n",
       " 'Бандере': 'бандера',\n",
       " 'звание': 'звание',\n",
       " 'Героя': 'герой',\n",
       " 'Украины': 'украина',\n",
       " 'обратились': 'обратиться',\n",
       " 'депутаты': 'депутат',\n",
       " 'Верховной': 'верховный',\n",
       " 'Рады': 'рада',\n",
       " 'Парламентарии': 'парламентарий',\n",
       " 'уверены': 'уверить',\n",
       " 'признание': 'признание',\n",
       " 'национальным': 'национальный',\n",
       " 'героем': 'герой',\n",
       " 'поможет': 'помочь',\n",
       " 'борьбе': 'борьба',\n",
       " 'подрывной': 'подрывной',\n",
       " 'деятельностью': 'деятельность',\n",
       " 'информационном': 'информационный',\n",
       " 'поле': 'поле',\n",
       " 'а': 'а',\n",
       " 'также': 'также',\n",
       " 'остановит': 'остановить',\n",
       " 'распространение': 'распространение',\n",
       " 'мифов': 'миф',\n",
       " 'созданных': 'создать',\n",
       " 'российской': 'российский',\n",
       " 'пропагандой': 'пропаганда',\n",
       " 'Степан': 'степан',\n",
       " '1909-1959': '1909-1959',\n",
       " 'был': 'быть',\n",
       " 'одним': 'один',\n",
       " 'из': 'из',\n",
       " 'лидеров': 'лидер',\n",
       " 'выступающей': 'выступать',\n",
       " 'за': 'за',\n",
       " 'создание': 'создание',\n",
       " 'независимого': 'независимый',\n",
       " 'государства': 'государство',\n",
       " 'территориях': 'территория',\n",
       " 'украиноязычным': 'украиноязычный',\n",
       " 'населением': 'население',\n",
       " '2010': '2010',\n",
       " 'году': 'год',\n",
       " 'период': 'период',\n",
       " 'президентства': 'президентство',\n",
       " 'Виктора': 'виктор',\n",
       " 'Ющенко': 'ющенко',\n",
       " 'посмертно': 'посмертно',\n",
       " 'признан': 'признать',\n",
       " 'Героем': 'герой',\n",
       " 'однако': 'однако',\n",
       " 'впоследствии': 'впоследствии',\n",
       " 'это': 'это',\n",
       " 'было': 'быть',\n",
       " 'отменено': 'отменить',\n",
       " 'судом': 'суд'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
    "\n",
    "natasha_lemmatize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rck5OVqhLVSA"
   },
   "source": [
    "### mystem vs. pymorphy vs. natasha\n",
    "\n",
    "1) *Мы надеемся, что вы пользуетесь линуксом*, но mystem работает невероятно медленно под windows на больших текстах.\n",
    "\n",
    "2) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту, natasha тоже с этим тоже не справляется успешно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "kH2GQ4ddLVSB"
   },
   "outputs": [],
   "source": [
    "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
    "homonym2 = 'Сорока своровала блестящее украшение со стола.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwF-XsjeI3eX",
    "outputId": "34e9f131-3af5-4a87-9a8c-aa51d6e076ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
      "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
     ]
    }
   ],
   "source": [
    "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
    "\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9jezRVlFmDo",
    "outputId": "e2224fd9-09e3-428e-fa6a-1af94dcb2ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'За': 'за', 'время': 'время', 'обучения': 'обучение', 'я': 'я', 'прослушал': 'прослушать', 'больше': 'большой', 'сорока': 'сорок', 'курсов': 'курс', '.': '.'}\n"
     ]
    }
   ],
   "source": [
    "print(natasha_lemmatize(homonym1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXjGBQPoI9gl",
    "outputId": "049fe8af-e0c5-499f-cc10-6d05b047a168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Сорока': 'сорок', 'своровала': 'своровать', 'блестящее': 'блестящий', 'украшение': 'украшение', 'со': 'с', 'стола': 'стол', '.': '.'}\n"
     ]
    }
   ],
   "source": [
    "print(natasha_lemmatize(homonym2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP5qFnilLVSI"
   },
   "source": [
    "## Словарь, закон Ципфа и закон Хипса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1umtd3OLVSI"
   },
   "source": [
    "Закон Ципфа -- эмпирическая закономерность: если все слова корпуса текста упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n. Иными словами, частотность слов убывает очень быстро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "lY0cWJ7eLVSJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIjqSVjpLVSL",
    "outputId": "a75ec748-ab21-4bd0-cd41-5a9fa8362b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "_oWC7NpkLVSO",
    "outputId": "965b9fbd-6328-4c13-f20f-cd3714d1adb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 69472),\n",
       " ('и', 55166),\n",
       " ('в', 52902),\n",
       " ('я', 52818),\n",
       " ('RT', 38070),\n",
       " ('на', 35759),\n",
       " ('http', 32998),\n",
       " ('что', 31541),\n",
       " ('с', 27217),\n",
       " ('а', 26860)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict = Counter(corpus)\n",
    "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
    "list(freq_dict_sorted)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "FrPkce0SLVSQ",
    "outputId": "d2ab5675-433a-480a-90ee-fa5dd9890922"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "first_100_freqs = [freq for word, freq in freq_dict_sorted[:100]]\n",
    "plt.plot(first_100_freqs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_N5V_K-LVSU"
   },
   "source": [
    "Закон Хипса -- обратная сторона закона Ципфа. Он описывает, что чем больше корпус, тем меньше новых слов добавляется с добавлением новых текстов. В какой-то момент корпус насыщается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw0GieJSMU-O"
   },
   "source": [
    "## Задание 1.\n",
    "\n",
    "**Задание**: обучите три классификатора: \n",
    "\n",
    "1) на токенах с высокой частотой \n",
    "\n",
    "2) на токенах со средней частотой \n",
    "\n",
    "3) на токенах с низкой частотой\n",
    "\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "QUQ6kAgPMqNn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gribanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "my_stopwords = stopwords.words('russian')\n",
    "# noise = stopwords.words('russian') + list(punctuation)\n",
    "noise = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226834 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "0       @first_timee хоть я и школота, но поверь, у на...  positive\n",
       "1       Да, все-таки он немного похож на него. Но мой ...  positive\n",
       "2       RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive\n",
       "3       RT @digger2912: \"Кто то в углу сидит и погибае...  positive\n",
       "4       @irina_dyshkant Вот что значит страшилка :D\\nН...  positive\n",
       "...                                                   ...       ...\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative\n",
       "\n",
       "[226834 rows x 2 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_importance(maxdf, mindf):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(max_df=maxdf, # не берем слова что выше тресхолда\n",
    "                                 min_df=mindf, # не берем что ниже тресхолда\n",
    "#                                  max_features=1000, \n",
    "#                                  stop_words=noise,\n",
    "                                 )\n",
    "    \n",
    "    bow = tfidf_vect.fit_transform(x_train)\n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    clf.fit(bow, y_train)\n",
    "    \n",
    "    pred = clf.predict(tfidf_vect.transform(x_test))\n",
    "    print(f'max_df= {maxdf} min_df= {mindf}')\n",
    "    print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df= 1 min_df= 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.14      0.58      0.23      6915\n",
      "    positive       0.90      0.51      0.65     49794\n",
      "\n",
      "    accuracy                           0.52     56709\n",
      "   macro avg       0.52      0.55      0.44     56709\n",
      "weighted avg       0.81      0.52      0.60     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Берем все токены - f1-score получается низкая\n",
    "get_tokens_importance(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df= 1.0 min_df= 0.285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.60      0.44     16283\n",
      "    positive       0.77      0.55      0.64     40426\n",
      "\n",
      "    accuracy                           0.56     56709\n",
      "   macro avg       0.56      0.57      0.54     56709\n",
      "weighted avg       0.65      0.56      0.58     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Берем токены часто встречающиеся\n",
    "get_tokens_importance(1.0, 0.285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df= 0.7 min_df= 0.15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.60      0.44     16029\n",
      "    positive       0.78      0.55      0.64     40680\n",
      "\n",
      "    accuracy                           0.56     56709\n",
      "   macro avg       0.56      0.57      0.54     56709\n",
      "weighted avg       0.66      0.56      0.58     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Берем токены средне встречающиеся\n",
    "get_tokens_importance(0.7, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df= 0.3 min_df= 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.77      0.75     26799\n",
      "    positive       0.79      0.75      0.77     29910\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Берем токены низкочастотные\n",
    "get_tokens_importance(0.3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по метрикам, низкочастотные токены наиболее важны для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV3fmzp-LVSU"
   },
   "source": [
    "## О важности эксплоративного анализа\n",
    "\n",
    "Но иногда пунктуация бывает и не шумом -- главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "qjkMxK9VLVSV",
    "outputId": "dfea56d5-4d92-4862-9788-29c8c8db29ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     28142\n",
      "    positive       1.00      1.00      1.00     28567\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2fRbUAvLVSX"
   },
   "source": [
    "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите фичи с самыми большими коэффициэнтами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "\n",
    "найти фичи с наибольшей значимостью, и вывести их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4027088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@', 'first_timee', 'хоть', 'я', 'и', 'школота', ',', 'но', 'поверь', ',']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
    "corpus = [token for tweet in df.text for token in word_tokenize(tweet)]\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(', 212404),\n",
       " (')', 194005),\n",
       " (',', 188295),\n",
       " (':', 177675),\n",
       " ('@', 149978),\n",
       " ('не', 69472),\n",
       " ('!', 66923),\n",
       " ('.', 57623),\n",
       " ('и', 55166),\n",
       " ('в', 52902)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict = Counter(corpus)\n",
    "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
    "list(freq_dict_sorted)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_JRuyuRLVSY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vtAyItvLVSb"
   },
   "source": [
    "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "uqH07o-7LVSc",
    "outputId": "fad0a24a-98ee-4f84-8782-495548eb0fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.51      0.64     48399\n",
      "    positive       0.17      0.58      0.26      8310\n",
      "\n",
      "    accuracy                           0.52     56709\n",
      "   macro avg       0.52      0.54      0.45     56709\n",
      "weighted avg       0.77      0.52      0.59     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = '!'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "uqH07o-7LVSc",
    "outputId": "fad0a24a-98ee-4f84-8782-495548eb0fb4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.55      0.51     24715\n",
      "    positive       0.61      0.54      0.57     31994\n",
      "\n",
      "    accuracy                           0.54     56709\n",
      "   macro avg       0.54      0.55      0.54     56709\n",
      "weighted avg       0.55      0.54      0.55     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = ':'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "uqH07o-7LVSc",
    "outputId": "fad0a24a-98ee-4f84-8782-495548eb0fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92     33113\n",
      "    positive       0.83      1.00      0.91     23596\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.91      0.93      0.91     56709\n",
      "weighted avg       0.93      0.91      0.91     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = ')'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\")\" Это часть смайлика или сам смайлик - и его присутствие в твите в основном говорит о положительном твите и наоборот. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5THCOjMLVSg"
   },
   "source": [
    "## Символьные n-граммы\n",
    "\n",
    "Теперь в качестве фичей используем, например, униграммы символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "AIUwDOabLVSh",
    "outputId": "54f129b1-994f-448e-e861-1912b4a21cdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      1.00      1.00     28094\n",
      "    positive       1.00      0.99      1.00     28615\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_E0uPpgLVSj"
   },
   "source": [
    "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или инчае, на символах классифицировать тоже можно: для некторых задач (например, для определения языка) фичи-символьные n-граммы решительно рулят.\n",
    "\n",
    "Ещё одна замечательная особенность фичей-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готвых анализаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3.\n",
    "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
    "\n",
    "2) подобрать оптимальный размер для hashing векторайзера\n",
    "\n",
    "3) убедиться что для сетки нет переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.76      0.76     28192\n",
      "    positive       0.77      0.76      0.77     28517\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n",
      "Wall time: 9.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "\n",
    "pred = clf.predict(count_vect.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.77      0.75     26799\n",
      "    positive       0.79      0.75      0.77     29910\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n",
      "Wall time: 9.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "bow = tfidf_vect.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "\n",
    "pred = clf.predict(tfidf_vect.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина hash-вектора: 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.60      0.58     26814\n",
      "    positive       0.62      0.59      0.61     29895\n",
      "\n",
      "    accuracy                           0.59     56709\n",
      "   macro avg       0.59      0.59      0.59     56709\n",
      "weighted avg       0.60      0.59      0.59     56709\n",
      "\n",
      "Длина hash-вектора: 1000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.65      0.64     27122\n",
      "    positive       0.67      0.65      0.66     29587\n",
      "\n",
      "    accuracy                           0.65     56709\n",
      "   macro avg       0.65      0.65      0.65     56709\n",
      "weighted avg       0.65      0.65      0.65     56709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина hash-вектора: 10000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.72      0.71     27402\n",
      "    positive       0.73      0.71      0.72     29307\n",
      "\n",
      "    accuracy                           0.71     56709\n",
      "   macro avg       0.71      0.71      0.71     56709\n",
      "weighted avg       0.71      0.71      0.71     56709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина hash-вектора: 100000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.75      0.74     26979\n",
      "    positive       0.77      0.74      0.75     29730\n",
      "\n",
      "    accuracy                           0.74     56709\n",
      "   macro avg       0.74      0.75      0.74     56709\n",
      "weighted avg       0.75      0.74      0.75     56709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gribanov\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина hash-вектора: 1000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.76      0.74     26788\n",
      "    positive       0.78      0.74      0.76     29921\n",
      "\n",
      "    accuracy                           0.75     56709\n",
      "   macro avg       0.75      0.75      0.75     56709\n",
      "weighted avg       0.75      0.75      0.75     56709\n",
      "\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "feature_sizes = [100, 1000, 10000, 100000, 1000000]\n",
    "\n",
    "for feature_size in feature_sizes:\n",
    "    \n",
    "    h_vect = HashingVectorizer(n_features=feature_size)\n",
    "    h_vect.fit(x_train)\n",
    "\n",
    "    xtrain_count =  h_vect.transform(x_train)\n",
    "    xtest_count =  h_vect.transform(x_test)\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(xtrain_count, y_train)\n",
    "    predictions = classifier.predict(xtest_count)\n",
    "    #predictions\n",
    "    print(f'Длина hash-вектора: {feature_size}')\n",
    "    print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Conv1D, GRU, LSTM, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # формирует словарь как CountVect и TF-IDF \n",
    "# из унгикальных токенов и заменяет кажддый токен его айдишником "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Привеедм y к 0,1\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw in train_data.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16,), dtype=string, numpy=\n",
       " array([b'@ksyunich98 \\xd0\\xb0 \\xd1\\x8f \\xd0\\xb2\\xd0\\xbe\\xd1\\x82 \\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xba\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xb8 \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd0\\xbe\\xd1\\x8f\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe \\xd1\\x81 \\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd0\\xb5\\xd1\\x84\\xd0\\xbe\\xd0\\xbd\\xd0\\xb0 \\xd1\\x81\\xd0\\xb8\\xd0\\xb6\\xd1\\x83, \\xd0\\xbc\\xd0\\xb0\\xd0\\xbc\\xd0\\xb0 \\xd0\\xba\\xd0\\xbe\\xd0\\xbc\\xd0\\xbf \\xd0\\xb7\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xbc\\xd0\\xb0\\xd0\\xb5\\xd1\\x82, \\xd0\\xb1\\xd0\\xbb\\xd0\\xb8\\xd0\\xbd, \\xd0\\xba\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd1\\x87\\xd0\\xb5\\xd1\\x80\\xd1\\x82\\xd0\\xb0 \\xd0\\xbd\\xd0\\xbe\\xd1\\x83\\xd1\\x82 \\xd1\\x81\\xd0\\xbb\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0\\xd0\\xbb\\xd1\\x81\\xd1\\x8f?:D',\n",
       "        b'\\xd0\\x98 \\xd0\\xba\\xd0\\xb0\\xd0\\xba\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd1\\x84\\xd0\\xb8\\xd0\\xb3\\xd0\\xb0 \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd0\\xbb\\xd0\\xb5 \\xd1\\x82\\xd0\\xb0\\xd0\\xba\\xd0\\xb8\\xd1\\x85 \\xd1\\x84\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd0\\xba \\xd0\\x90\\xd0\\xb9\\xd0\\xb7\\xd0\\xb0 \\xd1\\x81 \\xd0\\x93\\xd1\\x83\\xd1\\x84\\xd0\\xbe\\xd0\\xbc \\xd0\\xb7\\xd0\\xb0\\xd1\\x8f\\xd0\\xb2\\xd0\\xbb\\xd1\\x8f\\xd1\\x8e\\xd1\\x82, \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\xbd\\xd0\\xb5 \\xd1\\x81\\xd1\\x85\\xd0\\xbe\\xd0\\xb4\\xd1\\x8f\\xd1\\x82\\xd1\\x81\\xd1\\x8f?! :(((( http://t.co/umr2z4LMB6',\n",
       "        b'\\xd1\\x82\\xd1\\x83\\xd1\\x82\\xd1\\x82\\xd0\\xb0 \\xd0\\xbb\\xd0\\xb0\\xd1\\x80\\xd1\\x81\\xd0\\xb5\\xd0\\xbd \\xd0\\xbd\\xd0\\xb0\\xd0\\xbe\\xd0\\xb1\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd1\\x82: \\xd0\\xbd\\xd0\\xb5 \\xd1\\x81\\xd1\\x80\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd1\\x82\\xd1\\x82\\xd1\\x83\\xd1\\x82) http://t.co/QSxOCyR98E',\n",
       "        b'\\xd0\\xad\\xd1\\x82\\xd0\\xbe \\xd1\\x87\\xd1\\x91 \\xd1\\x82\\xd0\\xb5\\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd1\\x8c, \\xd1\\x8f \\xd1\\x81\\xd0\\xbc\\xd0\\xbe\\xd0\\xb3\\xd1\\x83 \\xd0\\xb2 \\xd0\\xb3\\xd0\\xb0\\xd0\\xb7\\xd0\\xb5\\xd1\\x82\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd1\\x87\\xd0\\xb8\\xd1\\x82\\xd0\\xb0\\xd1\\x82\\xd1\\x8c \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\x98\\xd1\\x80\\xd0\\xba\\xd0\\xb0 \\xd0\\x9f\\xd1\\x83\\xd0\\xbf\\xd0\\xba\\xd0\\xb8\\xd0\\xbd\\xd0\\xb0 \\xd0\\xb1\\xd0\\xbb\\xd1\\x8f\\xd0\\xb4\\xd1\\x8c?!? \\xd0\\x9d\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0\\xd0\\xb4\\xd0\\xbe \\xd1\\x82\\xd0\\xb0\\xd0\\xba((( http://t.co/XhcNeBtKBf',\n",
       "        b'RT @Abstergo_obj16: \\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xb3\\xd0\\xbe\\xd0\\xb2\\xd0\\xbb\\xd1\\x8f \\xd0\\xbb\\xd1\\x8e\\xd0\\xb4\\xd1\\x8c\\xd0\\xbc\\xd0\\xb8 \\xd0\\xb2 \\xd0\\xba\\xd0\\xbe\\xd0\\xbd\\xd1\\x84\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd1\\x86\\xd0\\xb2\\xd0\\xb5\\xd1\\x82\\xd0\\xb0\\xd0\\xb5\\xd1\\x82...',\n",
       "        b'\\xd1\\x8f \\xd0\\xbd\\xd0\\xb5 \\xd1\\x85\\xd0\\xbe\\xd1\\x87\\xd1\\x83 \\xd1\\x82\\xd0\\xb5\\xd1\\x80\\xd1\\x8f\\xd1\\x82\\xd1\\x8c 2\\xd1\\x85 \\xd0\\xb1\\xd0\\xbb\\xd0\\xb8\\xd0\\xb7\\xd0\\xba\\xd0\\xb8\\xd1\\x85 \\xd0\\xbc\\xd0\\xbd\\xd0\\xb5 \\xd0\\xbb\\xd1\\x8e\\xd0\\xb4\\xd0\\xb5\\xd0\\xb9(((( \\xd0\\xbd\\xd0\\xbe \\xd1\\x8f \\xd0\\xb8\\xd0\\xbc \\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd0\\xbd\\xd0\\xbe\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0\\xd1\\x81\\xd1\\x82\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xba\\xd0\\xbe \\xd1\\x82\\xd0\\xb5\\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd1\\x8c \\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd1\\x82\\xd0\\xb8\\xd0\\xb2\\xd0\\xbd\\xd0\\xb0,\\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd1\\x8f \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x8e,\\xd1\\x81\\xd0\\xbc\\xd0\\xbe\\xd0\\xb6\\xd0\\xb5\\xd0\\xbc \\xd0\\xbb\\xd0\\xb8 \\xd0\\xbc\\xd1\\x8b \\xd1\\x82\\xd0\\xb5\\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd1\\x8c \\xd0\\xbe\\xd0\\xb1\\xd1\\x89\\xd0\\xb0\\xd1\\x82\\xd1\\x8c\\xd1\\x81\\xd1\\x8f...',\n",
       "        b'@DmitryPavlovich \\xd0\\xba\\xd0\\xbe\\xd0\\xbc\\xd0\\xbf\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f \\xd1\\x81 \\xd0\\xba\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xb9 \\xd1\\x8f \\xd1\\x82\\xd1\\x83\\xd1\\x82 \\xd0\\xb5\\xd0\\xb5 \\xd0\\xbb\\xd1\\x8e\\xd0\\xb1\\xd0\\xb8\\xd1\\x82 \\xd1\\x80\\xd0\\xb0\\xd0\\xbc\\xd1\\x83:(',\n",
       "        b'@good_old_lad \\xd1\\x85\\xd0\\xbe\\xd0\\xb4\\xd0\\xb8 \\xd0\\xbf\\xd0\\xb5\\xd1\\x88\\xd0\\xba\\xd0\\xbe\\xd0\\xbc) \\xd0\\xb8 \\xd0\\xb5\\xd1\\x89\\xd0\\xb5 \\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd1\\x8c \\xd0\\xb2 \\xd1\\x86\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd1\\x80\\xd0\\xb5 \\xd0\\xbc\\xd1\\x83\\xd0\\xb7\\xd1\\x8b\\xd0\\xba\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x8b\\xd0\\xb9 \\xd0\\xbc\\xd0\\xb0\\xd0\\xb3\\xd0\\xb0\\xd0\\xb7\\xd0\\xb8\\xd0\\xbd, \\xd0\\xbd\\xd0\\xbe \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbc\\xd0\\xbd\\xd1\\x8e \\xd0\\xbd\\xd0\\xb0\\xd0\\xb7\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f - \\xd1\\x82\\xd0\\xb0\\xd0\\xbc \\xd0\\xbe\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd1\\x8c \\xd0\\xb4\\xd0\\xb5\\xd1\\x88\\xd0\\xb5\\xd0\\xb2\\xd1\\x8b\\xd0\\xb5 \\xd0\\xb4\\xd0\\xb8\\xd1\\x81\\xd0\\xba\\xd0\\xb8, \\xd0\\xbf\\xd0\\xbe 2,5 \\xd0\\xb5\\xd0\\xb2\\xd1\\x80\\xd0\\xbe, \\xd1\\x8d\\xd1\\x82\\xd0\\xbe \\xd0\\xbf\\xd0\\xbe\\xd1\\x87\\xd1\\x82\\xd0\\xb8',\n",
       "        b'RT @kaxemybowuvy: \\xd0\\x9d\\xd1\\x83 \\xd0\\xb2\\xd0\\xbe\\xd1\\x82, \\xd0\\xb2\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xb5\\xd1\\x82\\xd0\\xb8\\xd0\\xbb\\xd0\\xb8\\xd1\\x81\\xd1\\x8c \\xd1\\x81\\xd0\\xbe \\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd0\\xb7\\xd0\\xbe\\xd1\\x87\\xd0\\xbd\\xd0\\xb8\\xd0\\xba\\xd0\\xbe\\xd0\\xbc. \\xd0\\x90\\xd0\\xb3\\xd0\\xb0, \\xd1\\x81 \\xd0\\xbd\\xd0\\xb8\\xd0\\xbc. \\xd0\\xa1 \\xd0\\x93\\xd1\\x83\\xd0\\xb4\\xd0\\xb5\\xd0\\xb7\\xd0\\xbe\\xd0\\xbc. :) \\xd0\\xbf\\xd0\\xbe\\xd0\\xbf\\xd0\\xb8\\xd0\\xbb\\xd0\\xb8 \\xd0\\xbf\\xd0\\xb8\\xd0\\xb2\\xd0\\xba\\xd0\\xb0, \\xd0\\xbf\\xd0\\xbe\\xd0\\xb5\\xd0\\xbb\\xd0\\xb8 \\xd1\\x81\\xd1\\x8b\\xd1\\x80\\xd0\\xbe\\xd0\\xb9 \\xd1\\x80\\xd1\\x8b\\xd0\\xb1\\xd1\\x8b... \\xd0\\x9d\\xd0\\xbe \\xd0\\xb7\\xd0\\xb0\\xd0\\xb2\\xd1\\x82\\xd1\\x80\\xd0\\xb0 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x80\\xd0\\xb0\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd1\\x83, \\xd1\\x82\\xd0\\xb0\\xd0\\xba',\n",
       "        b'@UpsetSoul @klaveziaaa \\xd0\\xbc\\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbd\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd1\\x81\\xd1\\x8c \\xd0\\xba\\xd0\\xbb\\xd1\\x83\\xd0\\xb1\\xd0\\xbd\\xd0\\xb8\\xd1\\x87\\xd0\\xbd\\xd0\\xbe\\xd0\\xb5 \\xd0\\xbc\\xd0\\xbe\\xd1\\x85\\xd0\\xb8\\xd1\\x82\\xd0\\xbe(',\n",
       "        b'RT @tufuzicojih: \\xd0\\xba\\xd0\\xbb\\xd1\\x91\\xd0\\xb2\\xd0\\xbe, \\xd0\\xbe\\xd0\\xbd \\xd1\\x83\\xd0\\xb6\\xd0\\xb5 \\xd1\\x83\\xd0\\xbc\\xd0\\xb5\\xd0\\xb5\\xd1\\x82 \\xd0\\xba\\xd0\\xbe\\xd0\\xbc\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd1\\x8b. \\xd0\\xbd\\xd0\\xbe \\xd0\\xb2\\xd0\\xbe\\xd1\\x82 \\xd0\\xb1\\xd1\\x8b\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xbe \\xd1\\x80\\xd0\\xb0\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd0\\xb0\\xd1\\x82\\xd1\\x8c \\xd0\\xb2\\xd1\\x81\\xd1\\x91 \\xd0\\xb5\\xd1\\x89\\xd1\\x91 \\xd0\\xbd\\xd0\\xb5 \\xd1\\x83\\xd0\\xbc\\xd0\\xb5\\xd0\\xb5\\xd1\\x82. \\xd0\\xba\\xd1\\x80\\xd0\\xbe\\xd0\\xbc\\xd0\\xb5 \\xd0\\xba\\xd0\\xb0\\xd0\\xba \\xd0\\xbd\\xd0\\xb0 \\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8b\\xd1\\x85 \\xd0\\xbc\\xd0\\xb0\\xd0\\xba\\xd0\\xb0\\xd1\\x85 :) \\xd1\\x89\\xd0\\xb0 \\xd0\\xbf\\xd0\\xbe\\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb1\\xd1\\x83\\xd1\\x8e \\xd0\\xbf\\xd1\\x80\\xd0\\xbe',\n",
       "        b'\\xd0\\xbf\\xd0\\xbe\\xd0\\xbd\\xd0\\xb0\\xd0\\xbf\\xd0\\xb8\\xd1\\x81\\xd0\\xb0\\xd0\\xbb\\xd0\\xb8 \\xd0\\xbc\\xd0\\xbe\\xd0\\xb4\\xd1\\x83\\xd0\\xbb\\xd0\\xb5\\xd0\\xb9 \\xd0\\xbf\\xd0\\xbe\\xd0\\xb4 \\xd1\\x8d\\xd1\\x82\\xd0\\xbe\\xd1\\x82 \\xd0\\xb4\\xd1\\x80\\xd1\\x83\\xd0\\xbf\\xd0\\xb0\\xd0\\xbb, \\xd0\\xb4\\xd0\\xb0\\xd0\\xb6\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xba\\xd1\\x80\\xd0\\xb0\\xd1\\x81\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbb\\xd0\\xb0\\xd0\\xb7\\xd0\\xb8\\xd1\\x82\\xd1\\x8c \\xd1\\x82\\xd0\\xbe\\xd0\\xbb\\xd0\\xba\\xd0\\xbe\\xd0\\xbc, \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xb2\\xd0\\xb8\\xd0\\xbb \\xd0\\xb8 \\xd1\\x80\\xd0\\xb0\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd0\\xb0\\xd0\\xb5\\xd1\\x82, \\xd0\\xb0 \\xd1\\x80\\xd0\\xb0\\xd0\\xbd\\xd1\\x8c\\xd1\\x88\\xd0\\xb5 \\xd1\\x81\\xd0\\xb8\\xd0\\xb4\\xd0\\xb5\\xd0\\xbb, \\xd0\\xba\\xd0\\xbe\\xd0\\xb4 \\xd0\\xbf\\xd0\\xb8\\xd1\\x81\\xd0\\xb0\\xd0\\xbb.. :(',\n",
       "        b'\\xd1\\x8f \\xd1\\x82\\xd0\\xb0\\xd0\\xba \\xd0\\xb1\\xd1\\x8b\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xbe \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb2\\xd1\\x8b\\xd0\\xba\\xd0\\xb0\\xd1\\x8e \\xd0\\xba \\xd0\\xbb\\xd1\\x8e\\xd0\\xb4\\xd1\\x8f\\xd0\\xbc,\\xd0\\xb0 \\xd0\\xbe\\xd1\\x82\\xd0\\xb2\\xd1\\x8b\\xd0\\xba\\xd0\\xb0\\xd1\\x8e \\xd0\\xb3\\xd0\\xbe\\xd0\\xb4\\xd0\\xb0\\xd0\\xbc\\xd0\\xb8..\\xd1\\x8f \\xd1\\x81\\xd0\\xba\\xd1\\x83\\xd1\\x87\\xd0\\xb0\\xd1\\x8e \\xd0\\xbf\\xd0\\xbe \\xd0\\xbd\\xd0\\xb8\\xd0\\xbc((.. \\xd0\\xba\\xd1\\x82\\xd0\\xbe-\\xd1\\x82\\xd0\\xbe \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd0\\xbe \\xd0\\xb7\\xd0\\xb0\\xd0\\xb1\\xd1\\x8b\\xd0\\xbb..\\xd0\\xb8\\xd1\\x81',\n",
       "        b':) \\xd0\\xa1 \\xd0\\xb4\\xd0\\xbe\\xd0\\xb1\\xd1\\x80\\xd1\\x8b\\xd0\\xbc \\xd1\\x83\\xd0\\xbb\\xd1\\x8b\\xd0\\xb1\\xd1\\x87\\xd0\\xb8\\xd0\\xb2\\xd1\\x8b\\xd0\\xbc \\xd1\\x83\\xd1\\x82\\xd1\\x80\\xd0\\xbe\\xd0\\xbc! http://t.co/LSvygzBNYP',\n",
       "        b'RT @zmey__kasta: \\xe2\\x80\\x9c@airat_13: @zmey__kasta \\xd1\\x82\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xba\\xd0\\xbe \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xbb \\xc2\\xab\\xd1\\x81\\xd0\\xb0\\xd0\\xbc\\xd1\\x8b\\xd0\\xbc \\xd1\\x81\\xd1\\x87\\xd0\\xb0\\xd1\\x81\\xd1\\x82\\xd0\\xbb\\xd0\\xb8\\xd0\\xb2\\xd1\\x8b\\xd0\\xbc \\xd1\\x87\\xd0\\xb5\\xd0\\xbb\\xd0\\xbe\\xd0\\xb2\\xd0\\xb5\\xd0\\xba\\xd0\\xbe\\xd0\\xbc \\xd0\\xbd\\xd0\\xb0 \\xd0\\x97\\xd0\\xb5\\xd0\\xbc\\xd0\\xbb\\xd0\\xb5\\xc2\\xbb!!! \\xd0\\x94\\xd0\\xb5\\xd0\\xb2\\xd0\\xbe\\xd1\\x87\\xd0\\xba\\xd0\\xb0. !!!)))\\xe2\\x80\\x9d \\xd0\\x9c\\xd0\\xbe\\xd0\\xb8 \\xd0\\xbf\\xd0\\xbe\\xd0\\xb7\\xd0\\xb4\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f!',\n",
       "        b'@JennyPhelps2 \\xd0\\xb2 \\xd0\\xbe\\xd0\\xb1\\xd1\\x89\\xd0\\xb5\\xd0\\xbc, \\xd0\\xbd\\xd0\\xbe\\xd1\\x80\\xd0\\xbc\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe. \\xd0\\xa2\\xd0\\xb0\\xd0\\xbc \\xd0\\xb7\\xd0\\xb0\\xd0\\xb4\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 - \\xd1\\x81\\xd0\\xbe\\xd1\\x87\\xd0\\xb8\\xd0\\xbd\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5-\\xd0\\xbc\\xd0\\xb8\\xd0\\xbd\\xd0\\xb8\\xd0\\xb0\\xd1\\x82\\xd1\\x8e\\xd1\\x80\\xd0\\xb0. \\xd0\\xaf \\xd0\\xb2\\xd0\\xb4\\xd0\\xbe\\xd1\\x85\\xd0\\xbd\\xd0\\xbe\\xd0\\xb2\\xd0\\xb8\\xd0\\xbb\\xd1\\x81\\xd1\\x8f \\xd0\\xb8 \\xd0\\xbd\\xd0\\xb0\\xd0\\xbf\\xd0\\xb8\\xd1\\x81\\xd0\\xb0\\xd0\\xbb \\xd0\\xbd\\xd0\\xb0 2!! \\xd0\\xbb\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd0\\xb0 :DDD'],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(16,), dtype=int32, numpy=array([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1])>)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "vocab_size = 10000 # выбрали 10000 а там было 31к\n",
    "seq_len = 100 # сколько токенов в тексте -проходят по всем текстам и выбирают максимум, или берут квантильное - \n",
    "# могут быть выбросы по длинам \n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int', # что каждый токен будем переводить в индекс относительно нашего словаря vocab_size\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)\n",
    "\n",
    "embedding_dim=200 # 200-мерные вектора будут у каждого токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim, name=\"embedding\")\n",
    "        self.conv1 = Conv1D(200, (3))\n",
    "        self.conv2 = Conv1D(200, (3))\n",
    "        self.gPool = GlobalAveragePooling1D()\n",
    "        self.fc1 = Dense(100, activation='relu')\n",
    "        self.fc2 = Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        x1 = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gPool((x + x1)/2)\n",
    "        x = self.fc1(x)\n",
    "#         x = self.ss(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "# тут подход что все выделено в класс - ти удобнее писать в таком подходе!!!!\n",
    "# так как еслп будут повторться слои "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = myNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10633/10633 [==============================] - 184s 17ms/step - loss: 0.4023 - accuracy: 0.7662 - val_loss: 0.3562 - val_accuracy: 0.7959\n",
      "Epoch 2/5\n",
      "10633/10633 [==============================] - 183s 17ms/step - loss: 0.3411 - accuracy: 0.8086 - val_loss: 0.3547 - val_accuracy: 0.7961\n",
      "Epoch 3/5\n",
      "10633/10633 [==============================] - 188s 18ms/step - loss: 0.3203 - accuracy: 0.8217 - val_loss: 0.3616 - val_accuracy: 0.7999\n",
      "Epoch 4/5\n",
      "10633/10633 [==============================] - 188s 18ms/step - loss: 0.3019 - accuracy: 0.8343 - val_loss: 0.3771 - val_accuracy: 0.7932\n",
      "Epoch 5/5\n",
      "10633/10633 [==============================] - 181s 17ms/step - loss: 0.2818 - accuracy: 0.8480 - val_loss: 0.3939 - val_accuracy: 0.7904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23421b8d5e0>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переобучение уже пошло с 3-й эпохи - и дальше стало только усиливаться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mmodel.predict(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.6599103e-01],\n",
       "       [ 1.6081847e-02],\n",
       "       [ 1.0182415e+01],\n",
       "       ...,\n",
       "       [-3.7103333e+01],\n",
       "       [ 1.4859509e+00],\n",
       "       [ 2.7088312e+01]], dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приведем preds в бинарный вид\n",
    "preds = [int(i[0]>0) for i in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81     28567\n",
      "           1       0.80      0.82      0.81     28142\n",
      "\n",
      "    accuracy                           0.81     56709\n",
      "   macro avg       0.81      0.81      0.81     56709\n",
      "weighted avg       0.81      0.81      0.81     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pred = clf.predict(tfidf_vect.transform(x_test))\n",
    "print(classification_report(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gJABxhalLVQu",
    "IaQMCGHFLVQ6",
    "5AJk1B39LVRP",
    "RJlvqWuALVRs",
    "rck5OVqhLVSA",
    "mV3fmzp-LVSU",
    "H5THCOjMLVSg",
    "02s2Vh7MLVSj",
    "b1khxRFDLVSm",
    "sfUmWcAQLVSt",
    "BxvtN-3zLVS5",
    "gyrHhYkgLVTB"
   ],
   "name": "sem1_intro_common.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
